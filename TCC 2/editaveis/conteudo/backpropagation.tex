\textit{Backpropagation} é um algoritmo de aprendizagem normalmente aplicado a MLP's que visa a aprendizagem baseada em correção de erros. Isso ocorre devido à retropropagação dos erros de saída de uma RNA pelas camadas anteriores, para que assim sejam balanceados os pesos de entrada da RNA.
Este algoritmo possui três fases distintas \cite{netto2006}:

\begin{itemize}
    \item Fase 1: Propagação dos sinais \\ Os sinais são propagados juntamente com os valores de entrada por todos os neurônios da RNA a partir da entrada de dados até a camada de saída.
    \item Fase 2: Cálculo dos erros \\ Os erros são calculados nos neurônios de saída da RNA para que sejam retropropagados.
    \item Fase 3: Retropropagação dos erros \\ Os erros que são encontrados nas camadas de saída, são propagados de volta para as camadas anteriores rebalanceando os pesos de entrada nas camadas superiores da RNA. Dessa forma, a RNA poderá chegar a resultados mais exatos nas próximas interações.
\end{itemize}

As três fases descritas podem ser observadas na Figura 8.

\begin{figure}[ht]
        \centering
        \label{fig08}
            \includegraphics[keepaspectratio=true, scale=0.4]{editaveis/images/backprop.eps}
        \caption{Propagação de sinais de entrada e erros no \textit{Backpropagation}.}
        Fonte : \cite{buranajun2007}
\end{figure}

O pseudo algoritmo a seguir descreve uma maneira genérica de funcionamento do \textit{backpropagation}:

\begin{enumerate}[noitemsep]
    \item Inicialização dos pesos de todos os nós da RNA;
    \item Inicializar o padrão de entrara e sua respectiva saída esperada;
    \item Iniciar propagação do padrão por cada camada da RNA para que seja calculada a saída em cada nó (como mostrado na Figura 5);
    \item Comparar a saída esperada com a obtida através da RNA e calcular o erro cometido na camada de saída;
    \item Atualizar os pesos dos nós na camada de saída com base no erro calculado;
    \item Repetir até chegar à camada de entrada:
        \begin{itemize}[noitemsep]
            \item Calcular o erro cometido por cada nó da camada anterior e ponderar pelo erro do nó da camada atual e dos nós seguintes;
        \end{itemize}
    \item Repetir os passos 2, 3, 4, 5 e 6 até obter um erro mínimo ou até atingir um dado número de iterações.
\end{enumerate}

